name: rpi5-openwebui

# Переиспользуемые конфигурации
x-logging: &default-logging
  driver: "json-file"
  options:
    max-size: "1m"
    max-file: "1"

services:
  ollama:
    image: ${OLLAMA_IMAGE:-ollama/ollama:latest}
    container_name: ollama
    restart: unless-stopped
    expose:
      - "11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - backend-net
    healthcheck:
      test: [ "CMD-SHELL", "ollama list || exit 1" ]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          memory: 4G

  caddy:
    image: ${CADDY_IMAGE:-caddy:2.10}
    container_name: caddy
    restart: unless-stopped
    environment:
      - SEARXNG_BASE_URL=${SEARXNG_BASE_URL:-search.localhost}
      - OPENWEBUI_HOSTNAME=${OPENWEBUI_HOSTNAME:-ai.localhost}
      - CLIPROXY_HOSTNAME=${CLIPROXY_HOSTNAME:-proxy.localhost}
      - LETSENCRYPT_EMAIL=${LETSENCRYPT_EMAIL:-internal}
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./caddy/config:/etc/caddy
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - backend-net
    depends_on:
      openwebui:
        condition: service_healthy
      searxng:
        condition: service_healthy
      cli-proxy-api-plus:
        condition: service_healthy
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://caddy:2019/metrics || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    logging: *default-logging
    deploy:
      resources:
        limits:
          memory: 256M

  openwebui:
    image: ${OPENWEBUI_IMAGE:-ghcr.io/open-webui/open-webui:main}
    container_name: openwebui
    restart: unless-stopped
    expose:
      - "8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
      - ENABLE_CORS=${ENABLE_CORS:-false}
      - CORS_ALLOW_ORIGIN=${CORS_ALLOW_ORIGIN:-*}
      # OpenTelemetry monitoring
      - ENABLE_OTEL=${ENABLE_OTEL:-false}
      - ENABLE_OTEL_TRACES=${ENABLE_OTEL_TRACES:-false}
      - ENABLE_OTEL_METRICS=${ENABLE_OTEL_METRICS:-false}
      - OTEL_EXPORTER_OTLP_ENDPOINT=${OTEL_EXPORTER_OTLP_ENDPOINT:-http://grafana:4317}
      - OTEL_EXPORTER_OTLP_INSECURE=${OTEL_EXPORTER_OTLP_INSECURE:-true}
      - OTEL_SERVICE_NAME=${OTEL_SERVICE_NAME:-open-webui}
    volumes:
      - ./openwebui/data:/app/backend/data
    networks:
      - backend-net
    depends_on:
      ollama:
        condition: service_healthy
      cli-proxy-api-plus:
        condition: service_healthy
      searxng:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080/health" ]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  grafana:
    image: grafana/grafana:11.4.0
    container_name: grafana
    restart: unless-stopped
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
      - "4317:4317" # OTLP gRPC
      - "4318:4318" # OTLP HTTP
    environment:
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor,metricsSummary,traceToMetrics,traceqlSearch
      - GF_AUTH_ANONYMOUS_ENABLED=${GF_AUTH_ANONYMOUS_ENABLED:-true}
      - GF_AUTH_ANONYMOUS_ORG_ROLE=${GF_AUTH_ANONYMOUS_ORG_ROLE:-Admin}
      - GF_AUTH_DISABLE_LOGIN_FORM=${GF_AUTH_DISABLE_LOGIN_FORM:-false}
      - GF_INSTALL_PLUGINS=grafana-pyroscope-app
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/grafana.ini:/etc/grafana/grafana.ini
      - ./monitoring/grafana/datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
      - ./monitoring/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml
    networks:
      - backend-net
    depends_on:
      - mimir
      - loki
      - tempo
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M
    profiles:
      - monitoring

  # Prometheus-compatible metrics storage
  mimir:
    image: grafana/mimir:latest
    container_name: mimir
    restart: unless-stopped
    command:
      - -config.file=/etc/mimir.yaml
      - -target=all
    expose:
      - "8080" # HTTP API for Prometheus endpoints
      - "9009" # gRPC
    volumes:
      - mimir_data:/data
      - ./monitoring/mimir-config.yaml:/etc/mimir.yaml
    networks:
      - backend-net
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          memory: 512M
    profiles:
      - monitoring

  # Distributed tracing backend
  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    restart: unless-stopped
    command:
      - -config.file=/etc/tempo.yaml
    expose:
      - "3200"
      - "4317"
      - "4318"
    volumes:
      - tempo_data:/var/tempo
      - ./monitoring/tempo-config.yaml:/etc/tempo.yaml
    networks:
      - backend-net
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          memory: 512M
    profiles:
      - monitoring

  # Log aggregation
  loki:
    image: grafana/loki:latest
    container_name: loki
    restart: unless-stopped
    command:
      - -config.file=/etc/loki/local-config.yaml
    expose:
      - "3100"
    volumes:
      - loki_data:/loki
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml
    networks:
      - backend-net
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          memory: 384M
    profiles:
      - monitoring

  valkey:
    container_name: valkey
    image: docker.io/valkey/valkey:9.0.1-alpine3.23
    command: valkey-server --save 30 1 --loglevel warning
    restart: unless-stopped
    networks:
      - backend-net
    volumes:
      - valkey_data:/data
    logging: *default-logging
    healthcheck:
      test: [ "CMD", "valkey-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    deploy:
      resources:
        limits:
          memory: 128M

  searxng:
    container_name: searxng
    image: docker.io/searxng/searxng:latest
    restart: unless-stopped
    networks:
      - backend-net
    expose:
      - "8080"
    volumes:
      - ./searxng/config:/etc/searxng:rw
      - searxng_data:/var/cache/searxng:rw
    environment:
      - LANG=C.UTF-8
      - LC_ALL=C.UTF-8
      - SEARXNG_PUBLIC_INSTANCE=${SEARXNG_PUBLIC_INSTANCE:-false}
      - SEARXNG_LIMITER=${SEARXNG_LIMITER:-true}
      - SEARXNG_BASE_URL=https://search.${SEARXNG_BASE_URL:-search.localhost}/
      - SEARXNG_IMAGE_PROXY=${SEARXNG_IMAGE_PROXY:-false}
      - SEARXNG_SECRET=${SEARXNG_SECRET}
      - SEARXNG_VALKEY_URL=${SEARXNG_VALKEY_URL:-valkey://valkey:6379/0}
      - SEARXNG_DEBUG=${SEARXNG_DEBUG:-false}
      - SEARXNG_PORT=${SEARXNG_PORT:-8080}
      - SEARXNG_BIND_ADDRESS=${SEARXNG_BIND_ADDRESS:-0.0.0.0}
      - SEARXNG_METHOD=${SEARXNG_METHOD:-POST}
    logging: *default-logging
    depends_on:
      valkey:
        condition: service_healthy
    healthcheck:
      test:
        - "CMD-SHELL"
        - "python3 -c 'import urllib.request; req = urllib.request.Request(\"http://localhost:8080/healthz\"); req.add_header(\"X-Forwarded-For\", \"127.0.0.1\"); urllib.request.urlopen(req)'"
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 512M
  cli-proxy-api-plus:
    image: ${CLI_PROXY_IMAGE:-eceasy/cli-proxy-api-plus:latest}
    container_name: cli-proxy-api-plus
    restart: unless-stopped
    networks:
      - backend-net
    depends_on:
      ollama:
        condition: service_healthy
      searxng:
        condition: service_healthy
    expose:
      - "8317"
    ports:
      - "54545:54545"
      - "51121:51121"
    environment:
      - DEPLOY=${DEPLOY:-}
    volumes:
      - ./cli-proxy-api-plus/config/config.yaml:/CLIProxyAPI/config.yaml
      - cli-proxy_auths:/root/.cli-proxy-api
      - ./cli-proxy-api-plus/logs:/CLIProxyAPI/logs
    logging: *default-logging
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8317/ || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 512M

networks:
  backend-net:


volumes:
  ollama_data:
    name: ollama_data
    labels:
      com.backup: "true"
      com.backup.exclude: "models"
      com.service: "ollama"
  caddy_data:
    name: caddy_data
    labels:
      com.backup: "true"
      com.service: "caddy"
  caddy_config:
    name: caddy_config
    labels:
      com.backup: "true"
      com.service: "caddy"
  searxng_data:
    name: searxng_data
    labels:
      com.backup: "true"
      com.service: "searxng"
  valkey_data:
    name: valkey_data
    labels:
      com.backup: "true"
      com.service: "valkey"
  cli-proxy_auths:
    name: cli-proxy_auths
    labels:
      com.backup: "true"
      com.service: "cli-proxy-api-plus"

  # Monitoring volumes
  grafana_data:
    name: grafana_data
  mimir_data:
    name: mimir_data
  tempo_data:
    name: tempo_data
  loki_data:
    name: loki_data
